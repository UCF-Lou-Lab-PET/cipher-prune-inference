### server
>>> Evaluating Bert
-> Role: 1
-> Address: 127.0.0.1
-> Port: 8000
<<<

> Setup Linear
> HE instance initialized:
-> Poly Mod Degree: 8192
-> Coeff Mod: 54 54 55 55
-> Plaintext Mod: 536903681(29 bits)

> HE instance initialized:
-> Poly Mod Degree: 8192
-> Coeff Mod: 60 60 60
-> Plaintext Mod: 557057(19 bits)

> HE instance initialized:
-> Poly Mod Degree: 8192
-> Coeff Mod: 54 54 55 55
-> Plaintext Mod: 4295049217(32 bits)

> Setup NonLinear
> Loading and preprocessing weights on server
> [TIMING]: Loading Model takes: 0.925289sec
> [TIMING]: Model Preprocessing takes: 14.4134sec
> Bert intialized done!

==>> Inference sample #0
> Receive input cts from client
> --- Entering Attention Layers ---
-> Layer - 0: Linear #1 HE
-> Layer - 0: Linear #1 done HE
-> Layer - 0: Linear #2 HE
-> Layer - 0: Linear #2 HE done
-> Layer - 0: Linear #3 HE
-> Layer - 0: Linear #3 HE done
-> Layer - 0: Linear #4 HE
-> Layer - 0: Linear #4 HE done
-> Layer - 1: Linear #1 HE
-> Layer - 1: Linear #1 done HE
-> Layer - 1: Linear #2 HE
-> Layer - 1: Linear #2 HE done
-> Layer - 1: Linear #3 HE
-> Layer - 1: Linear #3 HE done
-> Layer - 1: Linear #4 HE
-> Layer - 1: Linear #4 HE done
-> Layer - 2: Linear #1 HE
-> Layer - 2: Linear #1 done HE
-> Layer - 2: Linear #2 HE
-> Layer - 2: Linear #2 HE done
-> Layer - 2: Linear #3 HE
-> Layer - 2: Linear #3 HE done
-> Layer - 2: Linear #4 HE
-> Layer - 2: Linear #4 HE done
-> Layer - 3: Linear #1 HE
-> Layer - 3: Linear #1 done HE
-> Layer - 3: Linear #2 HE
-> Layer - 3: Linear #2 HE done
-> Layer - 3: Linear #3 HE
-> Layer - 3: Linear #3 HE done
-> Layer - 3: Linear #4 HE
-> Layer - 3: Linear #4 HE done
-> Layer - 4: Linear #1 HE
-> Layer - 4: Linear #1 done HE
-> Layer - 4: Linear #2 HE
-> Layer - 4: Linear #2 HE done
-> Layer - 4: Linear #3 HE
-> Layer - 4: Linear #3 HE done
-> Layer - 4: Linear #4 HE
-> Layer - 4: Linear #4 HE done
-> Layer - 5: Linear #1 HE
-> Layer - 5: Linear #1 done HE
-> Layer - 5: Linear #2 HE
-> Layer - 5: Linear #2 HE done
-> Layer - 5: Linear #3 HE
-> Layer - 5: Linear #3 HE done
-> Layer - 5: Linear #4 HE
-> Layer - 5: Linear #4 HE done
-> Layer - 6: Linear #1 HE
-> Layer - 6: Linear #1 done HE
-> Layer - 6: Linear #2 HE
-> Layer - 6: Linear #2 HE done
-> Layer - 6: Linear #3 HE
-> Layer - 6: Linear #3 HE done
-> Layer - 6: Linear #4 HE
-> Layer - 6: Linear #4 HE done
-> Layer - 7: Linear #1 HE
-> Layer - 7: Linear #1 done HE
-> Layer - 7: Linear #2 HE
-> Layer - 7: Linear #2 HE done
-> Layer - 7: Linear #3 HE
-> Layer - 7: Linear #3 HE done
-> Layer - 7: Linear #4 HE
-> Layer - 7: Linear #4 HE done
-> Layer - 8: Linear #1 HE
-> Layer - 8: Linear #1 done HE
-> Layer - 8: Linear #2 HE
-> Layer - 8: Linear #2 HE done
-> Layer - 8: Linear #3 HE
-> Layer - 8: Linear #3 HE done
-> Layer - 8: Linear #4 HE
-> Layer - 8: Linear #4 HE done
-> Layer - 9: Linear #1 HE
-> Layer - 9: Linear #1 done HE
-> Layer - 9: Linear #2 HE
-> Layer - 9: Linear #2 HE done
-> Layer - 9: Linear #3 HE
-> Layer - 9: Linear #3 HE done
-> Layer - 9: Linear #4 HE
-> Layer - 9: Linear #4 HE done
-> Layer - 10: Linear #1 HE
-> Layer - 10: Linear #1 done HE
-> Layer - 10: Linear #2 HE
-> Layer - 10: Linear #2 HE done
-> Layer - 10: Linear #3 HE
-> Layer - 10: Linear #3 HE done
-> Layer - 10: Linear #4 HE
-> Layer - 10: Linear #4 HE done
-> Layer - 11: Linear #1 HE
-> Layer - 11: Linear #1 done HE
-> Layer - 11: Linear #2 HE
-> Layer - 11: Linear #2 HE done
-> Layer - 11: Linear #3 HE
-> Layer - 11: Linear #3 HE done
-> Layer - 11: Linear #4 HE
-> Layer - 11: Linear #4 HE done
-> Sharing Pooling and Classification params...
-> Layer - Pooling
-> Layer - Classification
> [TIMING]: linear1 takes 30.6224 sec
> [TIMING]: linear2 takes 4.41976 sec
> [TIMING]: linear3 takes 12.7736 sec
> [TIMING]: linear4 takes 13.3999 sec
> [TIMING]: softmax takes 4.9214 sec
> [TIMING]: pruning takes 1.01455 sec
> [TIMING]: mul v takes 13.7674 sec
> [TIMING]: gelu takes 4.31445 sec
> [TIMING]: ln_1 takes 5.35626 sec
> [TIMING]: ln_2 takes 5.2959 sec
> [TIMING]: tanh takes 0.124959 sec
> [TIMING]: repacking takes 6.4047 sec
> [TIMING]: gt_sub takes 3.7129 sec
> [TIMING]: shift takes 1.39952 sec
> [TIMING]: conversion takes 2.92423 sec
> [TIMING]: ln_share takes 0.00225847 sec
> [TIMING]: Pool/Class takes 0.606833 sec
> [NETWORK]: Linear 1 consumes: 11105771 bytes
> [NETWORK]: Linear 2 consumes: 9594432 bytes
> [NETWORK]: Linear 3 consumes: 37787904 bytes
> [NETWORK]: Linear 4 consumes: 9594432 bytes
> [NETWORK]: Softmax consumes: 2835802560 bytes
> [NETWORK]: GELU consumes: 4887429120 bytes
> [NETWORK]: Layer Norm 1 consumes: 1827998259 bytes
> [NETWORK]: Layer Norm 2 consumes: 1827997996 bytes
> [NETWORK]: Tanh consumes: 8724480 bytes
> [NETWORK]: Softmax * V: 9624892 bytes
> [NETWORK]: Pruning: 113983584 bytes
> [NETWORK]: Shift consumes: 668655616 bytes
> [NETWORK]: gt_sub consumes: 1358362624 bytes
> [NETWORK]: Pooling / C consumes: 310785796 bytes
> [NETWORK]: Linear 1 consumes: 23 rounds
> [NETWORK]: Linear 2 consumes: 24 rounds
> [NETWORK]: Linear 3 consumes: 24 rounds
> [NETWORK]: Linear 4 consumes: 24 rounds
> [NETWORK]: Softmax consumes: 86976 rounds
> [NETWORK]: GELU consumes: 33792 rounds
> [NETWORK]: Layer Norm 1 consumes: 83736 rounds
> [NETWORK]: Layer Norm 2 consumes: 83736 rounds
> [NETWORK]: Tanh consumes: 3520 rounds
> [NETWORK]: Softmax * V: 24 rounds
> [NETWORK]: Pruning: 458 rounds
> [NETWORK]: Shift consumes: 18368 rounds
> [NETWORK]: gt_sub consumes: 16864 rounds
> [NETWORK]: Pooling / C consumes: 988 rounds
-> End to end takes: 108.013sec



### client
>>> Evaluating Bert
-> Role: 2
-> Address: 127.0.0.1
-> Port: 8000
<<<

> Setup Linear
> HE instance initialized:
-> Poly Mod Degree: 8192
-> Coeff Mod: 54 54 55 55
-> Plaintext Mod: 536903681(29 bits)

> HE instance initialized:
-> Poly Mod Degree: 8192
-> Coeff Mod: 60 60 60
-> Plaintext Mod: 557057(19 bits)

> HE instance initialized:
-> Poly Mod Degree: 8192
-> Coeff Mod: 54 54 55 55
-> Plaintext Mod: 4295049217(32 bits)

> Setup NonLinear
> Bert intialized done!

==>> Inference sample #0
> Loading inputs
> Repacking to column
> Send to client
> --- Entering Attention Layers ---
-> Sharing Pooling and Classification params...
-> Layer - Pooling
-> Layer - Classification
> [TIMING]: linear1 takes 46.0178 sec
> [TIMING]: linear2 takes 4.45185 sec
> [TIMING]: linear3 takes 12.9883 sec
> [TIMING]: linear4 takes 13.4314 sec
> [TIMING]: softmax takes 4.92152 sec
> [TIMING]: pruning takes 1.01331 sec
> [TIMING]: mul v takes 13.8787 sec
> [TIMING]: gelu takes 4.31495 sec
> [TIMING]: ln_1 takes 5.35646 sec
> [TIMING]: ln_2 takes 5.29609 sec
> [TIMING]: tanh takes 0.125034 sec
> [TIMING]: repacking takes 6.71732 sec
> [TIMING]: gt_sub takes 2.9326 sec
> [TIMING]: shift takes 1.39975 sec
> [TIMING]: conversion takes 91.6177 sec
> [TIMING]: ln_share takes 0.00151639 sec
> [TIMING]: Pool/Class takes 0.606917 sec
> [NETWORK]: Linear 1 consumes: 28870722 bytes
> [NETWORK]: Linear 2 consumes: 18884376 bytes
> [NETWORK]: Linear 3 consumes: 18884376 bytes
> [NETWORK]: Linear 4 consumes: 75537504 bytes
> [NETWORK]: Softmax consumes: 2835802560 bytes
> [NETWORK]: GELU consumes: 4887429120 bytes
> [NETWORK]: Layer Norm 1 consumes: 1845764924 bytes
> [NETWORK]: Layer Norm 2 consumes: 1845764644 bytes
> [NETWORK]: Tanh consumes: 8724480 bytes
> [NETWORK]: Softmax * V: 33313132 bytes
> [NETWORK]: Pruning: 115756640 bytes
> [NETWORK]: Shift consumes: 668655616 bytes
> [NETWORK]: gt_sub consumes: 1358362624 bytes
> [NETWORK]: Pooling / C consumes: 306868212 bytes
> [NETWORK]: Linear 1 consumes: 23 rounds
> [NETWORK]: Linear 2 consumes: 24 rounds
> [NETWORK]: Linear 3 consumes: 24 rounds
> [NETWORK]: Linear 4 consumes: 24 rounds
> [NETWORK]: Softmax consumes: 86976 rounds
> [NETWORK]: GELU consumes: 33792 rounds
> [NETWORK]: Layer Norm 1 consumes: 83736 rounds
> [NETWORK]: Layer Norm 2 consumes: 83736 rounds
> [NETWORK]: Tanh consumes: 3520 rounds
> [NETWORK]: Softmax * V: 24 rounds
> [NETWORK]: Pruning: 458 rounds
> [NETWORK]: Shift consumes: 18368 rounds
> [NETWORK]: gt_sub consumes: 16864 rounds
> [NETWORK]: Pooling / C consumes: 988 rounds
-> End to end takes: 123.362sec